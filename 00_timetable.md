# Semester timetable

Below are assignment due dates and a record of the topics we covered each week this semester, updated after each week. See syllabus for planned topics.




## Assignment due dates
| Assignment                                          | Due                      |
| --------------------------------------------------- | ------------------------ |
| 1 (LOOCV)                                           | 31 Jan (Week 3 Fri)      |
| 2 (classification, KNN)                             | 13 Feb (Week 5 Thu)      |
| 3 (decision trees, bagging)                         | 25 Feb (Week 7 Tue)      |
| 4 (boosting)                                        | 14 Mar (Week 9 Fri)      |
| 5 (neural networks)                                 | 25 Apr (Week 13 Fri)     |
| Individual project presentations (finals week, TBA) | 5 May 1:30-4:00 PM (Mon) |
| Individual project code or paper                    | TBA (about 9 May)        |



## Discussions

| Lead | Week | Day  | Date.... | Paper |
| ---- | ---- | ---- | -------- | ----- |
| TBA  | TBA  | TBA  | TBA      | TBA   |
|      |      |      |          |       |



## Week 1

* Intro to class
* What is machine learning?
* Algorithms in data science: model, training, inference
* Statistical inference: accuracy of a trained model
* Regression and classification
* Machine learning in a nutshell, all of it!



## Week 2

* Polynomial model algorithm
* Optimization training algorithms
  * minimizing the training error
* Cross-validation (CV) inference algorithm
  * k-fold CV for regression
  * train-test split
  * mean squared error
* Tuning parameters



## Week 3

* Model algorithms
  * Smoothing splines
  * k nearest neighbors (KNN), regression and classification
* Training algorithm
  * regularization: penalized least squares
* CV inference algorithm
  * k-fold CV for classification
  * error rate
* Theory: bias-variance tradeoff



## Week 4

* Model algorithms
  * Decision tree models
  * Ensemble algorithms: bagging
* Training algorithms
  * Recursive binary partitioning for decision trees
* Inference algorithms
  * Tuning decision trees with CV



## Week 5

* Model algorithms
  * Ensemble algorithms: bagging, random forests
* Inference algorithms
  * Tuning bagging with CV
  * Tuning random forests with "out of bag" CV
  * Explainable machine learning: variable importance
* Parallel processing
* Using a compute server



## Week 6

* Model algorithms
  * Ensemble algorithm: boosted trees
* Training algorithms
  * Gradient descent
  * Stochastic gradient descent
  * Gradient boosting
  * Stochastic gradient boosting
  * Extreme gradient boosting, XGBoost
* Inference algorithms
  * Tuning strategies for multiple hyperparameters
* One hot encoding for categorical variables



## Week 7

* Model algorithms
  * Single layer neural networks
  * Architectures for regression, classification, multifunction
  * Deep learning: multilayer neural networks
  * Wide vs deep, expressiveness
* Training algorithm
  * Mini-batch stochastic gradient descent
  * Using Keras library



## Week 8

* Model algorithms
  * Deep learning: convolutional neural networks
* Training algorithms
  * Dropout regularization
  * Train-validate-test split
* Generalization & importance of data
  * Scope of inference
  * Test set leakage
  * Transfer learning, pre-trained models



## Weeks 9-15

* Reading and discussion: contemporary and emerging applications in ecology
* Individual project
